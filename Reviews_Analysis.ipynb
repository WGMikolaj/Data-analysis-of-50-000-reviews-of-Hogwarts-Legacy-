{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "lDAQV44i2Mu_"
      },
      "outputs": [],
      "source": [
        "#@title Imports\n",
        "\n",
        "!pip install gensim\n",
        "!pip install en_core_web_sm\n",
        "!pip install networkx\n",
        "!pip install scipy\n",
        "!pip install pyvis\n",
        "!pip install pyLDAvis\n",
        "\n",
        "from gensim.models import Phrases\n",
        "from gensim.corpora import Dictionary\n",
        "from gensim.models.ldamodel import LdaModel\n",
        "from gensim.models.coherencemodel import CoherenceModel\n",
        "from gensim.utils import simple_preprocess\n",
        "from gensim import corpora, models\n",
        "import pyLDAvis\n",
        "import pyLDAvis.gensim_models as gensimvis\n",
        "import networkx as nx\n",
        "import nltk\n",
        "import matplotlib.pyplot as plt\n",
        "from pyvis.network import Network\n",
        "from gensim.models.phrases import Phrases, Phraser\n",
        "import json\n",
        "import requests\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
        "from nltk.tokenize import sent_tokenize\n",
        "nltk.download('vader_lexicon')\n",
        "vader = SentimentIntensityAnalyzer()\n",
        "from gensim.parsing.preprocessing import STOPWORDS\n",
        "import json\n",
        "import requests as rq\n",
        "from bs4 import BeautifulSoup\n",
        "import time\n",
        "import spacy\n",
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "import gensim\n",
        "from gensim import corpora, models\n",
        "from gensim.corpora import Dictionary\n",
        "from gensim.models import LdaModel\n",
        "import unicodedata\n",
        "import json\n",
        "import os\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import sys"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "h9qBWXVf2tVH"
      },
      "outputs": [],
      "source": [
        "#@title NLTK Downloads\n",
        "import requests\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "\n",
        "## Get the AFINN dictionary from GitHub\n",
        "lexicon = requests.get(\"https://raw.githubusercontent.com/MikoBie/ids/main/notebooks/AFINN-111.txt\").text.strip().lower()\n",
        "## Split the string by new light sign -- \\n\n",
        "lexicon = [ w.strip() for w in lexicon.split(\"\\n\") ]\n",
        "## Convert a list of strings, where the word and value are separated by \\t\n",
        "## (tab), into a dictionary, where a word is a key and an integer is a value.\n",
        "lexicon = { word: int(score) for word, score in map(lambda x: x.split(\"\\t\"), lexicon) }\n",
        "\n",
        "## Download lexicons etc.\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "nltk.download(\"wordnet\")\n",
        "nltk.download('omw-1.4')\n",
        "\n",
        "## Get stopwords for english and initialize lemmatizer\n",
        "stop_words = stopwords.words('english')\n",
        "lemmatizer = WordNetLemmatizer()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 342,
      "metadata": {
        "cellView": "form",
        "id": "e0OYArKP2W72"
      },
      "outputs": [],
      "source": [
        "#@title LDA MyCorpus\n",
        "class MyCorpus:\n",
        "    \"\"\"\n",
        "    A class that represents a corpus and has usefull methods defined.\n",
        "\n",
        "    \"\"\"\n",
        "    \n",
        "    def __init__(self, path, key='content'):\n",
        "        \"\"\"\n",
        "        Reads from a JSON line file. Tokenizes and lemmatizes\n",
        "        the text under key. It writes out the new JSON line\n",
        "        file with a new field -- tokens.\n",
        "        Args:\n",
        "            path (str): a path to a JSON line.\n",
        "            key (str): a key with the content to lemmatize.\n",
        "        \"\"\"\n",
        "        self._path_original = path\n",
        "        self._key = key\n",
        "        self._dictionary = None\n",
        "        self._path = path.replace('.', '_NLP.')\n",
        "        with open(self._path, 'w') as file:\n",
        "            n = 1\n",
        "            for line in open(self._path_original, 'r'):\n",
        "                temp_dict = json.loads(line)\n",
        "                text_nlp = nlp(temp_dict[self._key])\n",
        "                temp_dict['tokens'] = []\n",
        "                for token in text_nlp:\n",
        "                    is_stop = token.is_stop or token.is_punct or token.is_space \\\n",
        "                        or token.is_bracket or token.is_currency or token.is_digit \\\n",
        "                            or token.is_quote or len(token) < 2\n",
        "                    if is_stop:\n",
        "                        continue\n",
        "                    else:\n",
        "                        temp_dict['tokens'].append(token.lemma_.lower())\n",
        "                file.write( json.dumps(temp_dict) + '\\n')\n",
        "                sys.stdout.write(f'\\rLine {n} processed')\n",
        "                n += 1\n",
        "                sys.stdout.flush()\n",
        "\n",
        "        \n",
        "    def set_dictionary(self, dictionary):\n",
        "        \"\"\"\n",
        "        Assigns a gensim.corpora.dictionary.Dictioanry object\n",
        "        to self._dictionary.\n",
        "\n",
        "        Args:\n",
        "            dictionary (gensim.corpora.dictionary.Dictionary): a dictionary\n",
        "            that stores the frequencies of unique tokens in the corpus.\n",
        "        \"\"\"\n",
        "        self._dictionary = dictionary\n",
        "\n",
        "    def get_tokens(self):\n",
        "        \"\"\"\n",
        "        It takes the path to a JSON line file with comments from Reddit and\n",
        "        returns a generator that yields tokens for each comment.\n",
        "\n",
        "        Yields:\n",
        "            list : list of tokens for a comment from Reddit. \n",
        "        \"\"\"\n",
        "        for doc in open(self._path, 'r'):\n",
        "            temp = json.loads(doc)\n",
        "            yield temp['tokens']\n",
        "    \n",
        "    def get_bow(self):\n",
        "        \"\"\"\n",
        "        It takes a dictionary with frequencies of unique tokens in the corpus\n",
        "        and for each list of tokens returns a list of tuples that denote the \n",
        "        id of a given token and its frequency in a given document.\n",
        "\n",
        "        Raises:\n",
        "            ValueError: if the dictionary was not assigned to self._dictionary.\n",
        "\n",
        "        Yields:\n",
        "            list : a list of tuples that denote the id of a given token and its\n",
        "            frequency in a given document.\n",
        "        \"\"\"\n",
        "        if self._dictionary:\n",
        "            for doc in self.get_tokens():\n",
        "                yield self._dictionary.doc2bow(doc)\n",
        "        else:\n",
        "            raise ValueError('Dictionary has the value of None')\n",
        "    \n",
        "    def __iter__(self):\n",
        "        \"\"\"\n",
        "        Yields:\n",
        "            list : a list of tuples that denote the id of a given token and\n",
        "            its frequency in a given document.\n",
        "        \"\"\"\n",
        "        for doc in self.get_bow():\n",
        "            yield doc\n",
        "\n",
        "    def get_topics(self, model):\n",
        "        \"\"\"\n",
        "        It takes a model and returns a generator that yields a mapping for each\n",
        "        comment from Reddit. Among other keys it returns the most probable topic\n",
        "        based on the LDA model provided and its probability.\n",
        "\n",
        "        Args:\n",
        "            model (gensim.models.ldamodel.LdaModel): Latent Dirchlet Allocation\n",
        "            model.\n",
        "\n",
        "        Yields:\n",
        "            dict : a mapping for each comment from Reddit. Among other keys it\n",
        "            returns the most prpobable topic based on the LDA model provided and\n",
        "            its probability. \n",
        "        \"\"\"\n",
        "        for doc in open(self._path, 'r'):\n",
        "            temp = json.loads(doc)\n",
        "            topics = model.get_document_topics(self._dictionary.doc2bow(temp['tokens']))\n",
        "            topic, prob = sorted( topics, key = lambda x: x[1], reverse=True )[0]\n",
        "            temp['topic'] = topic + 1\n",
        "            temp['topic_prob'] = prob\n",
        "            yield temp\n",
        "\n",
        "                \n",
        "class MyModel(LdaModel):\n",
        "    \"\"\"\n",
        "    Subclass of gensim.models.LdaModel.\n",
        "    \"\"\"\n",
        "    def get_coherence(self, corpus):\n",
        "        \"\"\"\n",
        "        Returns the average coherence measure for the given model.\n",
        "\n",
        "        Args:\n",
        "            corpus (MyCorpus): A corpus on which the model is computed. \n",
        "\n",
        "        Returns:\n",
        "            float: the average coherence measure for the given model.\n",
        "        \"\"\"\n",
        "        top_topics = self.top_topics(corpus)\n",
        "        return sum([t[1] for t in top_topics]) / len(top_topics)\n",
        "    \n",
        "    def get_top_tokens(self, corpus):\n",
        "        \"\"\"\n",
        "        Returns a list of dictionaries that depict the most probable\n",
        "        tokens for each topic.\n",
        "\n",
        "        Args:\n",
        "            corpus (MyCorpus): A corpus on which the model was computed.\n",
        "\n",
        "        Returns:\n",
        "            list: list of dicitionaries that depict the most probable \n",
        "            tokens fro each topic.\n",
        "        \"\"\"\n",
        "        top_tokens = self.top_topics(corpus)\n",
        "        return [ { key : value for value, key in t[0] } for t in top_tokens[:10] ]\n",
        "\n",
        "    \n",
        "    \n",
        "        \n",
        "def run_lda_models(corpus, dictionary, min_topics, max_topics, step = 1, **kwargs):\n",
        "    \"\"\"\n",
        "    Computes a sequence of lda models for a given corpus and dictionary. It prints\n",
        "    the coherence measure and number of topics to the screen. It writes out the\n",
        "    model to disk.\n",
        "\n",
        "    Args:\n",
        "        corpus (MyModel): A stream of document vectors or sparse matrix of shape (num_documents, num_terms).\n",
        "        dictionary (dict): a mapping that assigns id to unique tokens from the corpus.\n",
        "        min_topics (int): the smallest number of topics to compute.\n",
        "        max_topics (int): the highest number of topics to compute.\n",
        "        step (int, optional): the size of the break inbetween computed models. Defaults to 1.\n",
        "    \"\"\"\n",
        "    name = input(\"Please provide the name of the model\\n\")\n",
        "    temp = dictionary[0]\n",
        "    id2word = dictionary.id2token\n",
        "    if not os.path.exists('models'):\n",
        "        os.mkdir('models')\n",
        "    if not os.path.exists('png'):\n",
        "        os.mkdir('png')\n",
        "    for num_topic in range(min_topics, max_topics+1, step):\n",
        "        model = MyModel( corpus = corpus,\n",
        "                         id2word=id2word,\n",
        "                         alpha = 'asymmetric',\n",
        "                         eta = 'auto',\n",
        "                         iterations = 500,\n",
        "                         passes = 20,\n",
        "                         eval_every=None,\n",
        "                         num_topics=num_topic,\n",
        "                         random_state=1044,\n",
        "                         per_word_topics=True)\n",
        "        temp_dict = {}\n",
        "        temp_dict['name'] = name\n",
        "        temp_dict['num_topics'] =  num_topic\n",
        "        temp_dict['coherence'] = model.get_coherence(corpus = corpus)\n",
        "        path_name = os.path.join('models', name + '-' + str(num_topic))\n",
        "        model.save(path_name) \n",
        "        print(temp_dict)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 343,
      "metadata": {
        "cellView": "form",
        "id": "8IJwEFZU2VxZ"
      },
      "outputs": [],
      "source": [
        "#@title Basics\n",
        "data = []\n",
        "for line in open('Reviews.jl', 'r'):\n",
        "  data.append(json.loads(line))\n",
        "reviews = data[0]['reviews']\n",
        "reviews_corrected = []\n",
        "for i in reviews.items():\n",
        "    reviews_corrected.append(i[1])\n",
        "    \n",
        "positive_reviews = []\n",
        "negative_reviews = []\n",
        "for i in reviews_corrected:\n",
        "  if i['voted_up'] == True:\n",
        "    positive_reviews.append(i)\n",
        "  else:\n",
        "    negative_reviews.append(i)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 344,
      "metadata": {
        "cellView": "form",
        "id": "1mhgz9xE6W5_"
      },
      "outputs": [],
      "source": [
        "#@title Cleaning reviews\n",
        "import re\n",
        "from nltk.tokenize import word_tokenize\n",
        "\n",
        "Reviews_Content = []\n",
        "for i in reviews_corrected:\n",
        "  temp_dict = {}\n",
        "  temp_dict['content'] = i['review']\n",
        "  Reviews_Content.append(temp_dict)\n",
        "\n",
        "Negative_Reviews_Content = []\n",
        "for i in negative_reviews:\n",
        "  temp_dict = {}\n",
        "  temp_dict['content'] = i['review']\n",
        "  Negative_Reviews_Content.append(temp_dict)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QpcNOlJFwVWo",
        "cellView": "form"
      },
      "outputs": [],
      "source": [
        "#@title Relevant tokens\n",
        "\n",
        "#Cleaning and tokenizing into sentences\n",
        "List_of_tokens = []\n",
        "for d in Reviews_Content:\n",
        " for i in d.values():\n",
        "  i = re.sub(r'[^\\w\\s,!.\\'\\\"]', '', i)\n",
        "  i = i.replace('\\n', '')\n",
        "  tokens = sent_tokenize(i.lower())\n",
        "  temp_list = []\n",
        "  for token in tokens:\n",
        "      token = re.sub(r'\\d', '', token)\n",
        "      token = re.sub(r'[^\\w\\s]', '', token)\n",
        "      temp_list.append(token)\n",
        "  List_of_tokens.append(temp_list)\n",
        "List_of_tokens\n",
        "\n",
        "key_words = [\"trans\", \"transgender\", 'transsexual', \"transvestite\", \"transphobic\", \"transphobia\", \"lgbt\", \"lgbtq\", \"rights\", \"ideology\", \"jk\", \"rowling\", \"Sirona\", \"Ryan\", \"boycott\"]\n",
        "\n",
        "#Some people write \"j k\" instead of \"jk\" when refering to Rowling, so we have to first fix that:\n",
        "for i in range(len(List_of_tokens)):\n",
        "    sub_list = List_of_tokens[i]\n",
        "    for j in range(len(sub_list)):\n",
        "        s = sub_list[j]\n",
        "        if \"j\" in s and \"k\" in s and s.index(\"j\") < s.index(\"k\"):\n",
        "            s = s.replace(\" \", \"\") if \" j k \" in s else s\n",
        "            sub_list[j] = s\n",
        "\n",
        "#For each review I take only the tokens (sentences) that regard trans rights topic\n",
        "#As You can see, I am using a re function to make sure only the exact words are included,\n",
        "#The reason for that is that there is a lot of noise (eg. words like \"transparent\") otherwise.\n",
        "\n",
        "#Targetting relevant sentences\n",
        "Relevant_sentences = []\n",
        "for review in List_of_tokens:\n",
        "    temp_list = []\n",
        "    for sentence in review:\n",
        "        if any(re.search(r\"\\b{}\\b\".format(word), sentence) for word in key_words):\n",
        "            temp_list.append(sentence)\n",
        "            Relevant_sentences.append(temp_list)\n",
        "\n",
        "#Removing the duplicates\n",
        "Relevant_sentences = list(set(map(tuple, Relevant_sentences)))\n",
        "Relevant_sentences = [list(l) for l in Relevant_sentences]\n",
        "Relevant_sentences"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Dealing with noise\n",
        "\n",
        "#Deleting stopwords and other noise in the data\n",
        "stop_words = set(stopwords.words('english'))\n",
        "stop_words.add('game')\n",
        "stop_words.add('im')\n",
        "#top_words.add('trans')\n",
        "#top_words.add('jk')\n",
        "#stop_words.add('rowling')\n",
        "#stop_words.add('rowle')\n",
        "#stop_words.add('transphobic')\n",
        "\n",
        "\n",
        "filtered_list = []\n",
        "for i in Relevant_sentences:\n",
        "  temp_list = []\n",
        "  for string in i:\n",
        "    # Tokenize the string into words so I can remove stopwords\n",
        "    words = string.split()\n",
        "    # Filter out the stopwords from the words\n",
        "    filtered_words = [word for word in words if word not in stop_words]\n",
        "    # Join the filtered words back into a string\n",
        "    filtered_text = ' '.join(filtered_words)\n",
        "    # Update the value in the dictionary with the filtered text\n",
        "    temp_list.append(filtered_text)\n",
        "  filtered_list.append(temp_list)\n",
        "#Note1: I have to join them back into a string because the MyCorpus command tokenizes them again\n",
        "#Note2: I join them in a way to preserve the original structure.\n",
        "#As such most of the results will be composed of list containing one element\n",
        "#Because most reviews contained one sentence regarding the revelant topic\n",
        "#However if there were more sentences, the list will have more elements\n",
        "\n",
        "#Writing them into a dictionary\n",
        "#Now if there were more sentences, they will be connected together\n",
        "review_dict = {}\n",
        "for i, review in enumerate(filtered_list):\n",
        "  for content in review:\n",
        "    review_dict[i] = {\"id\": i, \"content\": content}\n",
        "\n",
        "#Saving the dictionary into a file\n",
        "with open(\"Relevant_reviews.jl\", \"w\") as f:\n",
        "    for review in review_dict.values():\n",
        "        f.write(json.dumps(review) + \"\\n\")\n",
        "review_dict"
      ],
      "metadata": {
        "cellView": "form",
        "id": "maXPiEaxbb4O"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "As we can see, 1539 reviews regard the topic of interest. However, there is still a considerable ammount of noise in the data, we will see whether LDA model will allow to better distinguish between noise and relevant topic."
      ],
      "metadata": {
        "id": "u_jqFN3F3MJT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Creating model\n",
        "\n",
        "corpus = MyCorpus(path = 'Relevant_reviews.jl')\n",
        "\n",
        "## Create the dictionary\n",
        "dictionary = Dictionary(corpus.get_tokens())\n",
        "\n",
        "## Filter out words that occur in less than 20 documents, or more than 50% of the documents\n",
        "dictionary.filter_extremes(no_below= 20, no_above=0.5)\n",
        "\n",
        "## Add the dictionary to the corpus\n",
        "corpus.set_dictionary(dictionary)\n",
        "\n",
        "run_lda_models(corpus = corpus, dictionary = dictionary, min_topics=2, max_topics=12)"
      ],
      "metadata": {
        "cellView": "form",
        "id": "H9wzFt5gJoxA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VTdRhsOxJhAr",
        "cellView": "form"
      },
      "outputs": [],
      "source": [
        "#@title Visualizing model\n",
        "\n",
        "## Read in the module. It requires providing\n",
        "## the name of the model we want to load.\n",
        "model_name = input('Provide the name of the model you would like to load:\\n')\n",
        "model_path = os.path.join('models', model_name)\n",
        "model = LdaModel.load(model_path)\n",
        "\n",
        "## Print out and write the figures with the most \n",
        "## probable tokens in each topic.\n",
        "list_top_tokens = model.get_top_tokens(corpus)\n",
        "for i in range(len(list_top_tokens)):\n",
        "    plt.barh(list(list_top_tokens[i].keys()), list(list_top_tokens[i].values()), align = 'center')\n",
        "    plt.xlim(0,.03)\n",
        "    plt.gca().invert_yaxis()\n",
        "    plt.title('Topic' + ' ' + str(i + 1))\n",
        "    plt.xlabel('Probability')\n",
        "    plt.savefig('png/' + 'topic' + str(i + 1))\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "64Ah0mLc6RvY"
      },
      "source": [
        "The results came out far better than expected. Since only sentences that include topic of interest were selected a high degree of intertwining was to be expected. Turns out that LDA still performed very well:\n",
        "\n",
        "**Topic 1 & 3** represent World of Harry Potter and are composed mostly of sentences that praise the setting of the game and the world that Rowling has written\n",
        "\n",
        "**Topic 2** regards the issue of trans rights and game's boycott\n",
        "\n",
        "Topic 2 selects reviews that represent the topic of interest and eliminates even more noise that was still present in the data.\n",
        "\n",
        "Could we possibly use sentiment scores to be even more certain that a given review represents our topic of interest? Let's see if there exists an optimal value of a review's sentiment that could function as a cut-off for a review to be classified as belonging to the topic of interest.\n",
        "\n",
        "(To proceed You have to choose a model with three topics)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@title How do the documents in those topics look like?\n",
        "\n",
        "# Creating a list of tuples containing the document index and its topic distribution\n",
        "doc_topic_dist = model.get_document_topics(corpus)\n",
        "doc_topic_tuples = list(enumerate(doc_topic_dist))\n",
        "\n",
        "# Grouping the documents by their most likely topic\n",
        "grouped_docs = {}\n",
        "for doc_id, topics in doc_topic_tuples:\n",
        "    topic = max(topics, key=lambda x: x[1])[0]\n",
        "    if topic not in grouped_docs:\n",
        "        grouped_docs[topic] = []\n",
        "    grouped_docs[topic].append((doc_id, topics[1]))\n",
        "\n",
        "# Obtaining top 10 documents for each topic\n",
        "top_docs_per_topic = {}\n",
        "for topic, docs in grouped_docs.items():\n",
        "    topic_docs = sorted(docs, key=lambda x: x[1], reverse=True)[:10]\n",
        "    top_docs_per_topic[topic] = [x[0] for x in topic_docs]\n",
        "\n",
        "\n",
        "Topic_1 = {id: review_dict[id]['content'] for id in top_docs_per_topic[0] if id in review_dict}\n",
        "Topic_2 = {id: review_dict[id]['content'] for id in top_docs_per_topic[1] if id in review_dict}\n",
        "Topic_3 = {id: review_dict[id]['content'] for id in top_docs_per_topic[2] if id in review_dict}\n",
        "\n",
        "print(\"Topic 1:\")\n",
        "for key, value in Topic_1.items():\n",
        "    print(value)\n",
        "print('\\n')\n",
        "\n",
        "print(\"Topic 2:\")\n",
        "for key, value in Topic_2.items():\n",
        "    print(value)\n",
        "print('\\n')\n",
        "\n",
        "print(\"Topic 3:\")\n",
        "for key, value in Topic_3.items():\n",
        "    print(value)\n",
        "print('\\n')"
      ],
      "metadata": {
        "id": "Dj8P5TACeiTk",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Sentiments for all reviews\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "Reviews_scores = []\n",
        "for item in Relevant_sentences:\n",
        "    temp_dict = {}\n",
        "    temp_dict['Content'] = item\n",
        "    Reviews_scores.append(temp_dict)\n",
        "\n",
        "sentiment_scores_all = []\n",
        "for review in Relevant_sentences:\n",
        "  strings_sentiment = []\n",
        "  for string in review:\n",
        "        score = (vader.polarity_scores(string))\n",
        "        sentiment = score['compound'] * (1 - score['neu'])\n",
        "        strings_sentiment.append(sentiment)\n",
        "  sentiment_scores_all.append(strings_sentiment)\n",
        "\n",
        "sentiment_list = []\n",
        "for i in sentiment_scores_all:\n",
        "  if len(i) > 1:\n",
        "    mean_score = np.mean(i)\n",
        "  else:\n",
        "    mean_score = i[0]\n",
        "  sentiment_list.append(mean_score)\n",
        "\n",
        "for i in range(len(Reviews_scores)):\n",
        "    Reviews_scores[i]['Sentiment'] = sentiment_list[i]\n",
        "Reviews_scores"
      ],
      "metadata": {
        "cellView": "form",
        "id": "VqL7ThKbz757"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Is the sentiment correlated with Topic relatedness?\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "Topic_relatedness = []\n",
        "for item, value in doc_topic_tuples:\n",
        "      Topic_relatedness.append(value[1][1].item()) #the .item() is needed as it was a numpy float and not a normal float\n",
        "\n",
        "#Scaling values to range from 0 to 1\n",
        "Topic_relatedness_scaled = []\n",
        "for i in Topic_relatedness:\n",
        "  i = (i + 1) /2\n",
        "  Topic_relatedness_scaled.append(i)\n",
        "\n",
        "for i in range(len(Reviews_scores)):\n",
        "  Reviews_scores[i]['Topic2_Relatedness'] = Topic_relatedness_scaled[i]\n",
        "Reviews_scores\n",
        "\n",
        "# Extract the values to be plotted from the dictionary\n",
        "x = Topic_relatedness_scaled\n",
        "y = sentiment_list\n",
        "\n",
        "# Create a scatterplot using Seaborn\n",
        "sns.scatterplot(x=x, y=y)\n",
        "\n",
        "# Set the title and labels for the plot\n",
        "plt.title(\"Scatter Plot of Data\")\n",
        "plt.xlabel(\"How much is a given review related to Topic 2\")\n",
        "plt.ylabel(\"Sentiment of a given review\")\n",
        "\n",
        "# Show the plot\n",
        "plt.show()"
      ],
      "metadata": {
        "cellView": "form",
        "id": "TtYTaYaJ-RD_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "NOTE: I have just realized that LDA displays Topics in a random order, as such the graph above may not represent the relevant topic. This is not however a problem for further analysis and I discuss the relevant graph below anyways"
      ],
      "metadata": {
        "id": "bLr_3EZ_7Aco"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The results are not very informative. I hoped the chance of a review belonging to topic 2 would have a higher correlation with the sentiment of a language used. Perhaps it is, but the correlation can not be so easily found because the stances are so polarized: one reviews are using positive language and others negative language when referring to the same topic of trans inclusion."
      ],
      "metadata": {
        "id": "O5tWJZ43Ezdb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now we have raviews that we can be quite certain that in fact represent our topic of interest. However, there is still a need to distinguish review's stance on the matter.\n",
        "\n",
        "The relationship between language sentiment and review's topic is complicated by opposing stances. Reviews like \"I hate lgbt ideology\" and \"I hate transphobia\" will have an identical sentiment value AND (nearly) identical chance of being detected as belonging to Topic 2, but in fact represent completly opposing stances\n",
        "\n",
        "How to determine what is the review's stance on the matter?\n",
        "\n",
        "There is a solution! Bigram and trigram modeling allow for taking under consideration the co-occurrence of words in pairs and triplets.\n"
      ],
      "metadata": {
        "id": "B-DW3Ovg8juQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Collocation analysis\n",
        "from nltk.collocations import BigramAssocMeasures, BigramCollocationFinder\n",
        "\n",
        "#Some reviews contain more than one sentence, so we need to first join them before tokenizing\n",
        "word_tokens = []\n",
        "for i in filtered_list:\n",
        "  temp_list = []\n",
        "  if len(i) > 1:\n",
        "      temp_list.append(' '.join(i))\n",
        "  else:\n",
        "      word_tokens.append(i)\n",
        "  if len(temp_list) > 0:\n",
        "    word_tokens.append(temp_list)\n",
        "    \n",
        "\n",
        "#Now tokenize each review\n",
        "word_tokens_proper = []\n",
        "for item in word_tokens:\n",
        "  for i in item:\n",
        "    word_tokens_proper.append(word_tokenize(i))\n",
        "word_tokens_proper\n",
        "\n",
        "#Now let's connect \"jk\" and \"rowling\"\n",
        "for lst in word_tokens_proper:\n",
        "    for i in range(len(lst)-1):\n",
        "        if lst[i] == \"jk\" and lst[i+1] == \"rowling\":\n",
        "            lst[i] = \"jkrowling\"\n",
        "            lst.pop(i+1)\n",
        "            break\n",
        "word_tokens_proper\n",
        "\n",
        "anchor_words = [\"trans\", \"transgender\", 'transsexual', \"transvestite\", \"transphobic\", \"transphobia\", \"lgbt\", \"lgbtq\", \"rights\", \"ideology\", \"jk\", \"rowling\", \"sirona\", \"ryan\", \"boycott\", \"woke\", \"sjw\", \"artist\"]\n",
        "tokens = [token for word_tokens_proper in word_tokens_proper for token in word_tokens_proper]\n",
        "\n",
        "finder = BigramCollocationFinder.from_words(tokens)\n",
        "finder.apply_freq_filter(4) # only consider bigrams that appear at least 5 times\n",
        "finder.apply_word_filter(lambda w: len(w) < 3) # ignore bigrams containing words with less than 3 characters\n",
        "\n",
        "def anchor_filter(w1, w2):\n",
        "    return w1 in anchor_words or w2 in anchor_words\n",
        "\n",
        "finder.apply_ngram_filter(anchor_filter)\n",
        "\n",
        "finder.apply_ngram_filter(anchor_filter)\n",
        "\n",
        "measures = BigramAssocMeasures()\n",
        "scored_bigrams = finder.score_ngrams(measures.chi_sq)\n",
        "top_bigrams = finder.nbest(measures.chi_sq, 30)\n",
        "scored_bigrams\n",
        "\n",
        "#ID's of bigrams representing different stances\n",
        "negative = [4, 7, 10, 19, 33, 43, 46, 55, 56, 83, 94]\n",
        "positive = [69, 73, 103, 122, 126, 136, 146]\n",
        "\n",
        "selected_bigrams = []\n",
        "for i in negative:\n",
        "    selected_bigrams.append(scored_bigrams[i])"
      ],
      "metadata": {
        "cellView": "form",
        "id": "BRbkTAti2dhb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Bigram modeling\n",
        "\n",
        "#Dictionary and corpus\n",
        "dictionary = Dictionary([tokens for tokens in word_tokens_proper])\n",
        "corpus = [dictionary.doc2bow(tokens) for tokens in word_tokens_proper]\n",
        "\n",
        "#LDA\n",
        "lda_model = LdaModel(corpus, num_topics= 2, id2word=dictionary, passes=10)"
      ],
      "metadata": {
        "cellView": "form",
        "id": "oYpCGuN215tc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Words in topics\n",
        "\n",
        "# Print the top 5 words for each topic\n",
        "for topic_id in range(lda_model.num_topics):\n",
        "    top_words = model.show_topic(topic_id, topn=10)\n",
        "    print(\"Topic {}: {}\".format(topic_id, top_words))"
      ],
      "metadata": {
        "cellView": "form",
        "id": "pKPdWG3kQaTD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Topic visualization\n",
        "\n",
        "vis_filtered = gensimvis.prepare(lda_model, corpus, dictionary)\n",
        "pyLDAvis.display(vis_filtered)"
      ],
      "metadata": {
        "cellView": "form",
        "id": "x06b1q6HYVse"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title End result, model validation\n",
        "\n",
        "# Creating a list of tuples containing the document index and its topic distribution\n",
        "doc_topic_dist = lda_model.get_document_topics(corpus)\n",
        "doc_topic_tuples = list(enumerate(doc_topic_dist))\n",
        "\n",
        "# Grouping the documents by their most likely topic\n",
        "grouped_docs = {}\n",
        "for doc_id, topics in doc_topic_tuples:\n",
        "    topic = max(topics, key=lambda x: x[1])[0]\n",
        "    if topic not in grouped_docs:\n",
        "        grouped_docs[topic] = []\n",
        "    grouped_docs[topic].append((doc_id, topics[0]))\n",
        "\n",
        "# Obtaining top 10 documents for each topic\n",
        "top_docs_per_topic = {}\n",
        "for topic, docs in grouped_docs.items():\n",
        "    topic_docs = sorted(docs, key=lambda x: x[1], reverse=True)[:10]\n",
        "    top_docs_per_topic[topic] = [x[0] for x in topic_docs]\n",
        "\n",
        "\n",
        "Topic_1 = {id: review_dict[id]['content'] for id in top_docs_per_topic[0] if id in review_dict}\n",
        "Topic_2 = {id: review_dict[id]['content'] for id in top_docs_per_topic[1] if id in review_dict}\n",
        "\n",
        "print(\"Topic 1:\")\n",
        "for key, value in Topic_1.items():\n",
        "    print(value)\n",
        "print('\\n')\n",
        "\n",
        "print(\"Topic 2:\")\n",
        "for key, value in Topic_2.items():\n",
        "    print(value)\n",
        "print('\\n')"
      ],
      "metadata": {
        "cellView": "form",
        "id": "KQpzCQBL2T2z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**In conclusion**\n",
        "\n",
        "To determine whether it is possible to predict review's topic, an LDA model was created and followed with sentiment analysis. Next, a bigram model based on collocation analysis was carried out. End results allow for differentiation between reviewer's stance on the topic of trans inclusion and boycott."
      ],
      "metadata": {
        "id": "0KziHsJk239c"
      }
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}